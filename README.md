# Invoice Field-Value Extraction using LayoutLMv3 and FATURA Dataset

## Project Overview

This project focuses on extracting structured field-value pairs from invoice images using the state-of-the-art multimodal transformer model **LayoutLMv3**. The goal is to automate invoice data extraction for fields like invoice number, vendor, date, total amount, tax, and address with high accuracy, leveraging both textual and spatial layout features of documents.

### Key Points

- **Model:** LayoutLMv3 (pretrained and fine-tuned for invoice field extraction)
- **Dataset:** FATURA — a rich collection of 10,000+ invoices with detailed bounding box annotations and ground-truth key-value pairs
- **Approach:** Convert original annotation format into BIO-tagged tokens aligned with OCR-extracted words, allowing token classification fine-tuning with LayoutLMv3
- **Evaluation:** Token-level precision, recall, and F1 on held-out test data

---

## Directory Structure

project-root/
│
├── data/
│ ├── fatura_raw/ # Raw FATURA dataset after download & extraction
│ │ ├── Images/ # Invoice images (JPEG)
│ │ └── invoices_dataset_final/
│ │ └── Annotations/
│ │ ├── Original_Format/ # Raw JSON annotations — highest quality source
│ │ ├── layoutlm_HF_format/
│ │ └── COCO_compatible_format/
│ └── fatura_hf/ # Processed Hugging Face dataset, BIO tags, 5k samples split
│
├── model_out/ # Directory for fine-tuned models & processor save
│ └── best/
│
├── scripts/ # All main scripts
│ ├── 1_download_fatura.py # Download & unzip FATURA dataset
│ ├── prepare_hf_from_original.py # Convert Original_Format JSONs to HF dataset with BIO tags (5k samples)
│ ├── 3_train.py # Fine-tune LayoutLMv3 on prepared dataset
│ ├── 4_evaluate.py # Evaluate trained model on test set (token-level metrics)
│ └── 5_infer_single.py # Single invoice image inference using pytesseract + LayoutLMv3
│
├── requirements.txt # Python dependencies for environment setup
└── README.md # This file


---

## Setup Instructions

### 1. Install Python dependencies

Recommend Python 3.8+ environment.  
Install required packages:

pip install -r requirements.txt


### 2. Download & Prepare Dataset

Download and unzip the FATURA dataset


Convert the high-quality `Original_Format` annotations into a BIO-tagged Hugging Face dataset (limit to 5,000 samples, split 3k train / 1k val / 1k test):

python scripts/prepare_hf_from_original.py


### 3. Train LayoutLMv3 Model

Fine-tune the model on your prepared dataset:


Set env var to reduce CUDA fragmentation, run on GPU 0

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
CUDA_VISIBLE_DEVICES=0 python scripts/3_train.py


### 4. Evaluate Model

Evaluate token-level precision, recall, and F1 on the test split:


python scripts/4_evaluate.py


### 5. Run Inference on Single Invoice Image

Run inference on a new single image, outputs extracted field-value pairs:



python scripts/5_infer_single.py path/to/invoice.jpg


---

## Notes and Recommendations

- The dataset preparation script only processes 5,000 invoices for efficient training. Adjust `DESIRED_TOTAL` in `prepare_hf_from_original.py` if you want more.
- BIO tags are generated by overlapping OCR tokens with annotated bounding boxes from FATURA’s `Original_Format`.
- GPU with at least ~4GB VRAM recommended for training.
- Ensure Tesseract OCR engine is installed on your system (`sudo apt install tesseract-ocr` on Ubuntu).
- The pipeline is extensible to other document types by adapting the annotation converter.

---

## References

- [LayoutLMv3 Paper (2022)](https://arxiv.org/abs/2204.08387)  
- FATURA Dataset: [Original dataset source provided by the user]  
- Hugging Face Transformers & Datasets: https://huggingface.co/docs/transformers/ and https://huggingface.co/docs/datasets/

---

Feel free to raise issues or contribute improvements!

---  

